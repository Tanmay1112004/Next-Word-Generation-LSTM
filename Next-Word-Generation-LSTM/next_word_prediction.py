# -*- coding: utf-8 -*-
"""Next_Word_Prediction.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1_R9ZBVg-kl5jfbIn3pzKNTIGiREUWIGg

# NEXT WORD PREDICTION USING SEQ2SEQ MODEL TENSORFLOW, KERAS, LSTM
"""

from google.colab import drive
drive.mount('/content/drive')

import pandas as pd
import tensorflow as tf
import numpy as np

df = pd.read_csv('/content/tmdb_5000_movies.csv')
df.head(3)

df = df['original_title']

df

movie_name = df.to_list()

movie_name

# Initializes a Tokenizer object from TensorFlow's Keras API. This tokenizer will be used to convert text data into sequences of integers.
tokenizer = tf.keras.preprocessing.text.Tokenizer()

#Fits the tokenizer on the provided movie names (movie_name). This step essentially builds the vocabulary based on the words in the movie names and assigns a unique integer index to each word.
tokenizer.fit_on_texts(movie_name)

#Converts the movie names into sequences of integers using the fitted tokenizer. Each word in the movie names is replaced by its corresponding integer index in the tokenizer's vocabulary. The resulting sequences are stored in the variable seq.
seq = tokenizer.texts_to_sequences(movie_name)

seq[:10]

tokenizer.word_index

X = []
y = []
total_words_dropped = 0

for i in seq:
    if len(i) > 1:
        for index in range(1, len(i)):
            X.append(i[:index])
            y.append(i[index])
    else:
        total_words_dropped += 1

print("Total Single Words Dropped are:", total_words_dropped)

"""- preparing the data for training a neural network model. It creates sequences of input-output pairs from the sequences generated earlier.

-X = [] and y = []: Initialize empty lists X and y to store input sequences and corresponding output sequences respectively.

- total_words_dropped = 0: Initialize a counter to keep track of the total number of single-word sequences dropped.

- The loop iterates over each sequence i in seq (which contains the sequences of integers representing movie names).

- total_words_dropped = 0: Initialize a counter to keep track of the total number of single-word sequences dropped.

-The loop iterates over each sequence i in seq (which contains the sequences of integers representing movie names).

- Inside the loop, if the length of the sequence i is greater than 1, it means the sequence has more than one word, so it generates input-output pairs by sliding a window over the sequence. For each word in the sequence except the last one (i[:index]), it appends the subsequence i[:index] to X and the next word (i[index]) to y.

-If the length of the sequence is 1, it means it contains only a single word. In this case, it increments the total_words_dropped counter.

-Finally, it prints the total number of single-word sequences dropped.

- The purpose of this code seems to be to generate training data for a model where the input is a sequence of words and the output is the next word in the sequence. By sliding a window over each sequence, it creates multiple training examples from each original sequence, except for sequences that consist of only one word, which are dropped.


"""

X[:10]

y[:10]

X = tf.keras.preprocessing.sequence.pad_sequences(X)

X

X = tf.keras.preprocessing.sequence.pad_sequences(X)
X = np.array(X)
X.shape

y = tf.keras.utils.to_categorical(y)

y

y.shape

vocab_size = len(tokenizer.word_index) + 1

vocab_size

model = tf.keras.Sequential([
    tf.keras.layers.Embedding(vocab_size, 14),
    tf.keras.layers.LSTM(100, return_sequences=True),
    tf.keras.layers.LSTM(100),
    tf.keras.layers.Dense(100, activation='relu'),
    tf.keras.layers.Dense(vocab_size, activation='softmax'),
])

model.summary()

model.compile(
    optimizer=tf.keras.optimizers.Adam(learning_rate=0.001),
    loss='categorical_crossentropy',
    metrics=['accuracy'])

model.fit(X, y, epochs=250)

model.save('nwp.h5')

import os
print("Current working directory:", os.getcwd())

vocab_array = np.array(list(tokenizer.word_index.keys()))

vocab_array

def make_prediction(text, n_words):
    for i in range(n_words):
        text_tokenize = tokenizer.texts_to_sequences([text])
        text_padded = tf.keras.preprocessing.sequence.pad_sequences(text_tokenize, maxlen=14)
        prediction = np.squeeze(np.argmax(model.predict(text_padded), axis=-1))
        prediction = str(vocab_array[prediction - 1])
        print(vocab_array[np.argsort(model.predict(text_padded)) - 1].ravel()[:-3])
        text += " " + prediction
    return text

make_prediction("cloudy", 10)

make_prediction("mars", 10)







# -*- coding: utf-8 -*-
"""
NEXT WORD PREDICTION USING SEQ2SEQ MODEL (LSTM)
Dataset: TMDB 5000 Movie Titles
Author: Tanmay + ChatGPT
"""

# ================== IMPORTS ==================
import pandas as pd
import numpy as np
import tensorflow as tf
import gradio as gr

# ================== LOAD DATA ==================
df = pd.read_csv("/content/tmdb_5000_movies.csv")
titles = df['original_title'].astype(str).tolist()
print(f"Loaded {len(titles)} movie titles")

# ================== TOKENIZATION ==================
tokenizer = tf.keras.preprocessing.text.Tokenizer()
tokenizer.fit_on_texts(titles)
sequences = tokenizer.texts_to_sequences(titles)

# Generate training data (X, y)
X, y = [], []
for seq in sequences:
    for i in range(1, len(seq)):
        X.append(seq[:i])
        y.append(seq[i])

# Pad sequences
X = tf.keras.preprocessing.sequence.pad_sequences(X)
y = tf.keras.utils.to_categorical(y, num_classes=len(tokenizer.word_index) + 1)

vocab_size = len(tokenizer.word_index) + 1
print(f"Vocabulary size: {vocab_size}")
print(f"Training samples: {X.shape[0]}")

# ================== BUILD MODEL ==================
model = tf.keras.Sequential([
    tf.keras.layers.Embedding(vocab_size, 50, input_length=X.shape[1]),
    tf.keras.layers.LSTM(128, return_sequences=True),
    tf.keras.layers.LSTM(128),
    tf.keras.layers.Dense(128, activation='relu'),
    tf.keras.layers.Dense(vocab_size, activation='softmax')
])

model.compile(optimizer="adam", loss="categorical_crossentropy", metrics=["accuracy"])
model.summary()

# ================== TRAIN MODEL ==================
history = model.fit(X, y, epochs=50, batch_size=128, verbose=1)

# Save model
model.save("next_word_model.h5")

# ================== PREDICTION FUNCTION ==================
reverse_word_index = {v: k for k, v in tokenizer.word_index.items()}

def predict_next_words(seed_text, num_words=5, top_k=3):
    """Generate next words from a given seed text"""
    output_text = seed_text
    for _ in range(num_words):
        token_seq = tokenizer.texts_to_sequences([output_text])[0]
        token_seq = tf.keras.preprocessing.sequence.pad_sequences([token_seq], maxlen=X.shape[1])

        preds = model.predict(token_seq, verbose=0)[0]
        top_indices = preds.argsort()[-top_k:][::-1]  # top-k predictions
        next_word = reverse_word_index.get(top_indices[0], "")
        output_text += " " + next_word
    return output_text

# ================== GRADIO FRONTEND ==================
def gradio_predict(seed, words, topk):
    return predict_next_words(seed, num_words=words, top_k=topk)

demo = gr.Interface(
    fn=gradio_predict,
    inputs=[
        gr.Textbox(label="Seed Text", placeholder="Enter starting words..."),
        gr.Slider(1, 20, value=5, step=1, label="Number of words to predict"),
        gr.Slider(1, 5, value=3, step=1, label="Top-k predictions considered")
    ],
    outputs=gr.Textbox(label="Predicted Sequence"),
    title="ðŸŽ¬ Next Word Prediction with LSTM",
    description="Train on TMDB movie titles. Enter a seed word to generate new titles!"
)

demo.launch(share=True)



# -*- coding: utf-8 -*-
"""
NEXT WORD PREDICTION USING SEQ2SEQ MODEL (LSTM)
Dataset: TMDB 5000 Movie Titles
Author: Tanmay + ChatGPT
"""

# ================== IMPORTS ==================
import pandas as pd
import numpy as np
import tensorflow as tf
import gradio as gr
import matplotlib.pyplot as plt

# ================== LOAD DATA ==================
df = pd.read_csv("/content/tmdb_5000_movies.csv")
titles = df['original_title'].astype(str).tolist()
print(f"Loaded {len(titles)} movie titles")

# ================== TOKENIZATION ==================
tokenizer = tf.keras.preprocessing.text.Tokenizer()
tokenizer.fit_on_texts(titles)
sequences = tokenizer.texts_to_sequences(titles)

# Generate training data (X, y)
X, y = [], []
for seq in sequences:
    for i in range(1, len(seq)):
        X.append(seq[:i])
        y.append(seq[i])

# Pad sequences
X = tf.keras.preprocessing.sequence.pad_sequences(X)
y = tf.keras.utils.to_categorical(y, num_classes=len(tokenizer.word_index) + 1)

vocab_size = len(tokenizer.word_index) + 1
print(f"Vocabulary size: {vocab_size}")
print(f"Training samples: {X.shape[0]}")

# ================== BUILD MODEL ==================
model = tf.keras.Sequential([
    tf.keras.layers.Embedding(vocab_size, 50, input_length=X.shape[1]),
    tf.keras.layers.LSTM(128, return_sequences=True),
    tf.keras.layers.LSTM(128),
    tf.keras.layers.Dense(128, activation='relu'),
    tf.keras.layers.Dense(vocab_size, activation='softmax')
])

model.compile(optimizer="adam", loss="categorical_crossentropy", metrics=["accuracy"])
model.summary()

# ================== TRAIN MODEL ==================
history = model.fit(X, y, epochs=50, batch_size=128, verbose=1, validation_split=0.1)

# Save model
model.save("next_word_model.h5")

# ================== PLOT TRAINING CURVES ==================
plt.figure(figsize=(12,5))

# Loss plot
plt.subplot(1,2,1)
plt.plot(history.history['loss'], label="Train Loss", color="blue")
plt.plot(history.history['val_loss'], label="Val Loss", color="orange")
plt.xlabel("Epochs")
plt.ylabel("Loss")
plt.title("Training vs Validation Loss")
plt.legend()

# Accuracy plot
plt.subplot(1,2,2)
plt.plot(history.history['accuracy'], label="Train Accuracy", color="green")
plt.plot(history.history['val_accuracy'], label="Val Accuracy", color="red")
plt.xlabel("Epochs")
plt.ylabel("Accuracy")
plt.title("Training vs Validation Accuracy")
plt.legend()

plt.show()

# ================== PREDICTION FUNCTION ==================
reverse_word_index = {v: k for k, v in tokenizer.word_index.items()}

def predict_next_words(seed_text, num_words=5, top_k=3):
    """Generate next words from a given seed text"""
    output_text = seed_text
    for _ in range(num_words):
        token_seq = tokenizer.texts_to_sequences([output_text])[0]
        token_seq = tf.keras.preprocessing.sequence.pad_sequences([token_seq], maxlen=X.shape[1])

        preds = model.predict(token_seq, verbose=0)[0]
        top_indices = preds.argsort()[-top_k:][::-1]  # top-k predictions
        next_word = reverse_word_index.get(top_indices[0], "")
        output_text += " " + next_word
    return output_text

# ================== GRADIO FRONTEND ==================
def gradio_predict(seed, words, topk):
    return predict_next_words(seed, num_words=words, top_k=topk)

demo = gr.Interface(
    fn=gradio_predict,
    inputs=[
        gr.Textbox(label="Seed Text", placeholder="Enter starting words..."),
        gr.Slider(1, 20, value=5, step=1, label="Number of words to predict"),
        gr.Slider(1, 5, value=3, step=1, label="Top-k predictions considered")
    ],
    outputs=gr.Textbox(label="Predicted Sequence"),
    title="ðŸŽ¬ Next Word Prediction with LSTM",
    description="Train on TMDB movie titles. Enter a seed word to generate new titles!"
)

demo.launch(share=True)



"""DS lecture
Projects complete
Time series projects complete
Python 1 hr
sql 1hr
job application sending
"""















